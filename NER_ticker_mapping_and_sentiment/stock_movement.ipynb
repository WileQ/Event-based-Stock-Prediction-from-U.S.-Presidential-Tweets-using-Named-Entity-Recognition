{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b542ead4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wikto\\Desktop\\Studia 4 semestr\\Natural Language Processing\\Project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer, pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f008af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "sent_tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\")\n",
    "sent_model = AutoModelForSequenceClassification.from_pretrained(\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\")\n",
    "sent_pipeline = pipeline(\"sentiment-analysis\", model=sent_model, tokenizer=sent_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a230f8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date:  2020-11-27\n",
      "Sentence: big tech and the fa ##ke news media have partnered to suppress . freedom of the press is gone , a thing of the past . that ’ s why they refuse to report the real facts and figures of the 2020 election or even , where ’ s hunter !  \n",
      "Entities (non-O): []\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "sentences = []\n",
    "gold_labels = []\n",
    "dates = []\n",
    "found = []\n",
    "\n",
    "with open(\"Data/test_with_predictions.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    combined_t = []\n",
    "    combined_l = []\n",
    "\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)\n",
    "\n",
    "    for line in reader:\n",
    "        token, ignor_label, label = line\n",
    "\n",
    "        if token[:5] == \"date:\" and ignor_label == \"O\" and label == \"\":\n",
    "            if combined_l:\n",
    "                sentence_tokens = combined_t[1:] if combined_t[0] == '' else combined_t\n",
    "                label_tokens = combined_l[1:] if combined_t[0] == '' else combined_l\n",
    "\n",
    "                sentences.append(sentence_tokens)\n",
    "                gold_labels.append(label_tokens)\n",
    "\n",
    "                lista = [tok for tok, lab in zip(sentence_tokens, label_tokens) if lab != 'O']\n",
    "                found.append(lista)\n",
    "\n",
    "                combined_t = []\n",
    "                combined_l = []\n",
    "\n",
    "            dates.append(token[5:])\n",
    "            continue\n",
    "\n",
    "        combined_t.append(token)\n",
    "        combined_l.append(label)\n",
    "\n",
    "    if combined_l:\n",
    "        sentence_tokens = combined_t[1:] if combined_t[0] == '' else combined_t\n",
    "        label_tokens = combined_l[1:] if combined_t[0] == '' else combined_l\n",
    "\n",
    "        sentences.append(sentence_tokens)\n",
    "        gold_labels.append(label_tokens)\n",
    "\n",
    "        lista = [tok for tok, lab in zip(sentence_tokens, label_tokens) if lab != 'O']\n",
    "        found.append(lista)\n",
    "\n",
    "print(\"Date:\", dates[0])\n",
    "print(\"Sentence:\", \" \".join(sentences[0]))\n",
    "print(\"Entities (non-O):\", found[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cba0f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2020-03-02\n",
      "['michelle', '@', 'fis', '##ch', '##bac', '##hm', '##n', '##7', 'is', 'running', 'for', 'congress', 'in', 'minnesota', '.', 'nicole', 'is', 'strong', 'on', 'crime', 'and', 'borders', ',', 'cutting', 'taxes', ',', 'your', '#', '2a', ',', 'love', '##s', 'our', 'military', ',', 'vet', '##s', ',', 'and', 'will', 'stand', 'with', 'our', 'great', 'farmers', '.', 'michelle', 'has', 'my', 'complete', 'and', 'total', 'endorsement', '!', '', '']\n",
      "['PER_B', 'O', 'ORG_B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'ORG_B', 'O', 'LOC_B', 'O', 'PER_B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'PER_B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['michelle', 'fis', 'congress', 'minnesota', 'nicole', 'michelle']\n"
     ]
    }
   ],
   "source": [
    "print(dates[5004])\n",
    "print(sentences[5004])\n",
    "print(gold_labels[5004])\n",
    "print(found[5004])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4946909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved to processed_outputagg.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "output_file = \"Data/processed_outputagg.csv\"\n",
    "\n",
    "with open(output_file, \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Date\", \"Sentence\", \"Found Entities\"])\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        writer.writerow([\n",
    "            dates[i] if i < len(dates) else \"\",  \n",
    "            \" \".join(sentences[i]),\n",
    "            str(found[i])  \n",
    "        ])\n",
    "\n",
    "print(f\"CSV saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b50ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Date                                           Sentence  \\\n",
      "0       2020-11-27  big tech and the fa ##ke news media have partn...   \n",
      "1       2020-11-16  the rate of rejected mail - in ball ##ot ##s i...   \n",
      "2       2020-11-16  georgia won ’ t let us look at the all importa...   \n",
      "3       2020-11-18  wo ##w ! governor kem ##p will hopefully see t...   \n",
      "4       2020-11-16  european countries are sad ##ly getting clo ##...   \n",
      "...            ...                                                ...   \n",
      "15175   2020-01-03  iran never won a war , but never lost a negoti...   \n",
      "15176   2020-01-01  thank you to the @ dc ##exa ##mine ##r washing...   \n",
      "15177   2020-01-01  one of my greatest honor ##s was to have gotte...   \n",
      "15178   2020-10-22  just signed an order to support the workers of...   \n",
      "15179   2020-10-22  suburban women want safety & amp ; security . ...   \n",
      "\n",
      "                                     Found Entities Matched Companies  \\\n",
      "0                                                []                []   \n",
      "1                                  ['pennsylvania']                []   \n",
      "2                                       ['georgia']                []   \n",
      "3                         ['kem', 'georgia', 'usa']                []   \n",
      "4                                         ['china']                []   \n",
      "...                                             ...               ...   \n",
      "15175                                      ['iran']                []   \n",
      "15176                                ['washington']                []   \n",
      "15177                                            []                []   \n",
      "15178  ['delphi', 'corporation', 'obama', 'delphi']                []   \n",
      "15179                                ['joe', 'bid']                []   \n",
      "\n",
      "      Matched Tickers  \n",
      "0                  []  \n",
      "1                  []  \n",
      "2                  []  \n",
      "3                  []  \n",
      "4                  []  \n",
      "...               ...  \n",
      "15175              []  \n",
      "15176              []  \n",
      "15177              []  \n",
      "15178              []  \n",
      "15179              []  \n",
      "\n",
      "[15180 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "df_text = pd.read_csv(\"Data/processed_outputagg.csv\")\n",
    "df_companies = pd.read_csv(\"../sp500_ticker_data_collection/ticker_data/sp500_with_appended_aliases.csv\")\n",
    "\n",
    "name_to_ticker = {}\n",
    "for _, row in df_companies.iterrows():\n",
    "    ticker = row['Ticker']\n",
    "    name_to_ticker[row['Company'].lower()] = ticker\n",
    "    try:\n",
    "        aliases = ast.literal_eval(row['Aliases'])\n",
    "        for alias in aliases:\n",
    "            name_to_ticker[alias.lower()] = ticker\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def match_entities(entities_str):\n",
    "    try:\n",
    "        entities = ast.literal_eval(entities_str)\n",
    "        matched_companies = []\n",
    "        matched_tickers = []\n",
    "        for entity in entities:\n",
    "            ticker = name_to_ticker.get(entity.lower())\n",
    "            if ticker:\n",
    "                matched_companies.append(entity)\n",
    "                matched_tickers.append(ticker)\n",
    "        return pd.Series([matched_companies, list(set(matched_tickers))])\n",
    "    except:\n",
    "        return pd.Series([[], []])\n",
    "\n",
    "df_text[['Matched Companies', 'Matched Tickers']] = df_text['Found Entities'].apply(match_entities)\n",
    "\n",
    "print(df_text[['Date', 'Sentence', 'Found Entities', 'Matched Companies', 'Matched Tickers']])\n",
    "df_text.to_csv(\"Data/matched_with_tickers.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9253006d",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = df_text['Matched Tickers'].explode().value_counts()\n",
    "\n",
    "c.to_csv(\"Data/ticker_counts.csv\", header=[\"Count\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fcb49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tweets = pd.read_csv(\"Data/matched_with_tickers.csv\")\n",
    "\n",
    "tweets['Sentence'] = tweets['Sentence'].str[2:-1]\n",
    "\n",
    "tweets['Matched Tickers'] = tweets['Matched Tickers'].apply(eval) \n",
    "filtered = tweets[tweets['Matched Tickers'].apply(lambda x: len(x) > 0)].copy()\n",
    "\n",
    "sent_list = []\n",
    "for sentence in filtered['Sentence']:\n",
    "    sent = sent_pipeline(sentence) \n",
    "    sent_list.append(sent)\n",
    "\n",
    "filtered['Sentiment'] = sent_list\n",
    "\n",
    "tweets = tweets.merge(filtered[['Sentence', 'Sentiment']], on='Sentence', how='left')\n",
    "\n",
    "tweets.to_csv(\"Data/processed_with_sentiment.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
